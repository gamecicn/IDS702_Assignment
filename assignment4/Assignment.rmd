---
title: "Assignment 4"
author: "Maobin Guo"
output:
  pdf_document: default
  word_document: default
  html_document:
    
    df_print: paged
---

```{r setup, echo = FALSE, include=FALSE}

options(warn=-1)
options(digits=4)
options(xtable.comment = FALSE)

library(ggplot2)
library(xtable)
library(mice)
library(VIM)
library(sjPlot)
library(car)

```

# Question I

```{r echo = FALSE, include=FALSE}
df <- read.csv("./data/treeage.txt", 
               stringsAsFactors = FALSE, 
                strip.white=TRUE)


df <- df[, -1]

summary(df)
str(df)
miss_data <- df

```
 
## Task 1.1 


```{r echo = FALSE, include=FALSE}
set.seed(95)
miss_data[head(sample(20), 0.3 * nrow(miss_data)), ]$age <- NA
```

The following code is used to replace 30% age with missing value randomly. 

```
missing_data[head(sample(20), 0.3 * nrow(df)), ]$age <- NA
```

This is the new dataset with missing value.

```{r echo = FALSE, results='asis', caption = "Dataset with missing value"}
xtable(miss_data)
```


```{r echo = FALSE, include=FALSE}
tree_imp <- mice(miss_data,
                 m=50, 
                 defaultMethod=c("norm","logreg","polyreg","polr"), 
                 print=F)

```

## Task 1.2 

```{r echo = FALSE, message=FALSE, out.width=c('50%'), fig.align='center'}

densityplot(tree_imp, 
            scales = list(x = list(relation = "free")), 
            ayout = c(2, 1),  
            main = "Marginplot Distribution of Multiple Imputation")
```

```{r echo = FALSE, message=FALSE, out.width=c('50%', '50%', '50%', '50%')}

for (imp_index in c(1:4)) {
  print(ggplot( data.frame(diameter=miss_data$diameter, 
                   age = complete(tree_imp, imp_index)$age,  
                   missing = is.na(miss_data$age) ), 
        aes(y = age, x = diameter,  color = missing)) +
        geom_point() +
        theme_classic() +
        ggtitle(sprintf("Missing value and observed value for imputed dataset : %d",                                imp_index)))
}

```


**Conclusion:**
According to the scatter plot, the result is acceptable since the added data are in a reasonable range. Most of the imputed ages are larger than the real value for a small diameter data point. However, one of the imputed datasets predicts a very close value of real age. According to marginal distribution, most of the imputed data's distribution is similar to observed data, except for a few too centered distributions. Considering the tiny size of the dataset, the few exceptionals are acceptable. 
 
## Task 1.3

The model is :

$$
age_{i} = \beta_{1}*diameter_{i} + \beta_{0} + \varepsilon_{i};\varepsilon \overset{iid}{\sim} \mathcal{N}(\mu,\,\sigma^{2})\
$$
**Model Assessment**
The assessment plot of one of the imputed datasets is as follows. According to residual analysis, there is no obvious evidence indicate the assumptions of linear regression were broken. There are some high leverage points; however,  there are not high influence data. Hence, these data points can be preserved in the model without worry.


```{r echo = FALSE, message=FALSE, out.width=c('50%', '50%', '50%', '50%')}

m1 <- lm(data=complete(tree_imp, 1), age~diameter)

plot(m1, which=1)
plot(m1, which=2)
plot(m1, which=3)
plot(m1, which=5)
```



**Conclusion** 


```{r echo = FALSE, message=FALSE, fig.align='center', results='asis', caption = "Coefficient-Level Estimates"}

reg_imp <- with(data=tree_imp, lm(age~diameter))

regm <-  pool(reg_imp)

xtable(summary(regm))
```

The table is the summary of the model applying on multiple imputation datasets. The diameter has positive effects on the age of trees because its p-value is significant. Suppose the diameter increase by 1 unit, the age of the tree would increase by 11 years. The intercept's p-value is near 1, which means it is not significant. If there is more data, then a better model can be obtained, in which intercept may be significant. 

\newpage

# PART II 

 
```{r echo = FALSE, message=FALSE, include=FALSE}
 
df <- read.csv("./data/nhanes.csv", 
               stringsAsFactors = FALSE, 
               strip.white=TRUE,
               na.strings = c("NA", "."))

drop_cols <- c('sdmvstra', 'sdmvpsu', 'wtmec2yr', 'ridageyr')
df <- df[, !(names(df) %in% drop_cols )]

df$riagendr = as.factor(df$riagendr)
df$ridreth2 = as.factor(df$ridreth2)
df$dmdeduc = as.factor(df$dmdeduc)
df$indfminc = as.factor(df$indfminc)

summary(df)
str(df)
```

## Task 2.1
 
```{r echo = FALSE, include=FALSE}
#full  <- mice(df,
#              m=10, 
#              defaultMethod=c("norm","logreg","polyreg","polr"), 
#              print=F)
#
#saveRDS(full, file = "part2_full.rds")

```

```{r echo = FALSE, include=FALSE}
full <- readRDS(file = "part2_full.rds")
```


```{r echo = FALSE, out.width=c('50%', '50%')}


#xyplot(full, 
#       bmxbmi ~ age | .imp,
#       pch=c(1,20),
#       cex = 1.4,
#       col=c("grey","darkred"),
#       main = "Scatter plot of bmxbmi vs age")
#
#xyplot(full, 
#       bmxbmi ~ riagendr | .imp,
#       pch=c(1,20),
#       cex = 1.4,
#       col=c("grey","darkred"),
#       main = "Scatter plot of bmxbmi vs riagendr")
#
#
#densityplot(full, 
#            scales = list(x = list(relation = "free")), 
#            ayout = c(2, 1),  
#            main = "Densityplot of Multiple Imputation")

```
**Plots: **

![](./meta/p2_sca_bmi_vs_age.png){height=50%}

![](./meta/p2_sca_bmi_vs_riagendr.png){height=50%}
![](./meta/p2_sca_density.png){height=80%}
 
**Conclusions:**

According to the plot scatter plots of bmxbmi by age and riagendr, I do not think the imputation is great. Because the imputation data (red points) clusters in a part of the observed data (gray points), which means some factors are ignored in this imputation method. Bmxbmi and bmxaml's marginal distributions show quite different distribution between imputation data and observed data.  Considering the dataset's size, the differences are considerable. The quality of the imputation model is not satisfying.


## Task 2.2

```{r echo = FALSE, message=FALSE, include = FALSE, fig.align='center', results='asis'}

#Build model

tdf <- complete(full, 1)
tdf$age2 <- tdf$age ^ 0.5

m1 <- lm(data = tdf, 
             log(bmxbmi) ~ age + riagendr + ridreth2 + dmdeduc + indfminc)
# summary(model1)

m2 <- lm(data = tdf, 
             log(bmxbmi) ~ age2 + riagendr + ridreth2 + dmdeduc + indfminc )
summary(m2)
m3 <- lm(data = tdf, 
             log(bmxbmi) ~ age2 + riagendr + ridreth2 + dmdeduc + indfminc +
                           age2:dmdeduc)
summary(m3)

anova(m2, m3)
#
#
#
#mn <- model2
# 
#
#mf <- lm(
#    log(bmxbmi) ~ (age1 + age2 + riagendr + ridreth2 + dmdeduc + indfminc) ^ 2,
#    data = tdf 
#  )
#
#Model_stepwise_aic <- step(mn,
#                           scope = mf,
#                           direction = "both",
#                           trace = 0)
#summary(Model_stepwise_aic)




# Other mordel for F-test ?


```

The model is 
$$
\begin{aligned}
log(bmxbmi_{i}) = & \beta_{1}*\sqrt{age}_{i} + \beta_{2}*riagendr_{i} + \beta_{3}*dmdeduc_{i}  + \\ 
                  & \beta_{4}*indfminc_{i} + \beta_{5}*dmdeduc_{i}:\sqrt{age}_{i} + \beta_{0} + \varepsilon_{i}; \\
                  & \varepsilon \overset{iid}{\sim} \mathcal{N}(\mu,\,\sigma^{2})\
\end{aligned}
$$

**Model Exploration**

```{r echo = FALSE, message=FALSE, fig.align='center', out.width=c('50%'), warning=FALSE}

ggplot(tdf, aes(x = age2, y = bmxbmi)) +
    geom_point(alpha = .5, colour = "blue4") +
    geom_smooth(col = "red3") +
    theme_classic() + 
    labs(title = "BMI vs Square of Age influenced by education") + 
    facet_wrap(~ dmdeduc)

```

In the model exploring stage, I tried to compare many models bu ANOVA test on a complete data set and finally decided to use the above model. After that, the model was applied to another data set, and a similar conclusion was reached. Then I decided to use the model to predict BMI. In the model fitting step, I found the log transformation could cause the response variable closer to the normal distribution. Moreover, the ANOVA test suggests that the model will perform better after transforming age to the power of 0.5. So I kept these two transformations in the final model.

The plot above illustrates the interaction between the square of age and education versus the response variable. The bim trend with the square of age is different according to education. This interaction is also confirmed in the model fitting step.

For the indfminc variable, I try to combine different categories, such as combining incomes of less than US$20,000 into a group and combining incomes of more than US$20,000 into a group. However, the ANOVA test suggested that this kind of merging will not improve the model's effect, so I finally gave up this kind of merging. I also used AIC, BIC did a model search in different directions, and the final result was the same as the above model. So I finally decided to use this model.

**Model Assessment**

From the residual plots, there are no obviously violations of assumptions:

1) Linearity: The residual versus predictor plot seems random. 

2) Independence and Equal Variance: Absence of any pattern, randomness and wide-spread distributions over the spectrum support these assumptions.

3) QQ-plot supports the assumption of Normality generally as the plot is a straight line 
   between 2 standardized residuals. 

Also, according to Cook's distance, there is no high leverage data point, which is good news for the imputed dataset. To ensure no multicollinearity, VIF scores were generated, noticing that all variables had VIF value below 5.  

```{r echo = FALSE, message=FALSE, out.width=c('50%', '50%', '50%', '50%')}

plot(m3, which=1)
plot(m3, which=2)
plot(m3, which=3)
plot(m3, which=4)
```

```{r echo = FALSE, message=FALSE, results='asis', caption = "VIF Check"}

xtable(vif(m3))

```

```{r echo = FALSE, message=FALSE, fig.align='center', results='asis',  caption = "Coefficient-Level Estimates"}

bmireg_imp <- with(data=full, m3)

bmireg_pool <-  pool(bmireg_imp)

xtable(summary(bmireg_pool))
```

**Conclusion**

The baseline values incorporated in the intercept are age = 0, male, Non-Hispanic white, less than high school education, and income below $4999. From the model summary, we found that age, gender, race, education, and the interaction between the square of age and education are strongly associated (statistically significant) with the BMI. Almost all income categories are statistically significant with BMI except indfminc9 ($55,000 to $64,999). Considering that no other income categories are statistically significant, and the coefficient of indfminc9 is small, I tend to consider income would not affect a person's BIM.

1. Controlling other factors, one year raise on the square of age would increase his/her BMI by 8.4%

2. Controlling other factors, the gender of female would raise BMI by 2.1%

3. All categories of race are statistically significant; however, their coefficient is small. Controlling other factors, when the race was changed from Non-Hispanic whites to Non-Hispanic black, Mexican American, other race and other Hispanic, their BMI would increase by 7%, 5% -4.3% 2.5% respectively. 

4. Controlling other factors, high school, and more than a high school diploma would increase BMI by 33% and 30%.

5. At the significance level of p<0.001, keeping other variables constant for every increase in the square of age, a person with a high school diploma his/her BMI tends to decrease by 3.8%.

6. At the significance level of p<0.001, keeping other variables constant for every increase in the square of age, a person with more than a high school diploma his/her BMI tends to decrease by 3.6%.



 
 
  


































